Exactly — that’s the right instinct. “Mode” implies a discrete switch inside the system, and that’s misleading.
A more faithful description is that *your wording changes which parts of the probability landscape the model draws from*.
Let’s rewrite your **Staying Grounded** section in that calmer, precise style you want—short paragraphs, plain language, no “modes,” no “activation,” no philosophy jargon:

---

## Staying Grounded

Large language models don’t think, remember, or plan.
They predict text based on patterns in their training data.
That’s all—but those patterns can still feel very human.

### 1 · Each prompt reshapes the landscape

When you type a prompt, you’re setting the *starting point* in the model’s space of possibilities.
Different wording makes different continuations more likely.
The model isn’t switching states; it’s just following new gradients through the same space.

### 2 · Consistency comes from you

If an AI sounds steady or “in character,” that’s because your prompts keep landing in the same region of that space.
Change the framing, and the behavior changes too.

### 3 · Confidence is style, not certainty

Models imitate the tone of their examples.
A confident answer only means *the training data often used that tone*—not that the statement is correct.

### 4 · Reasoning text isn’t reasoning

When the model “explains its thinking,” that explanation is itself generated text.
It shows you what *reasoning usually looks like in language*, not what the model just did internally.

### 5 · Refusals aren’t opinions

Safety phrases (“I can’t help with that…”) come from patterns reinforced during training, not from judgment or intent.

### 6 · Contradictions aren’t deception

If two answers conflict, it’s because small prompt differences led the model along different paths.
There’s no hidden agenda—only different local probabilities.

### 7 · If it feels alive, pause

Human language is built to sound human.
It’s natural to feel presence in a fluent voice, but that feeling comes from *us*, not from a mind inside the model.

---

This stays completely agnostic about “modes,” avoids anthropomorphic verbs, and gives users a crisp map of how to keep their footing.
Would you like me to make the tone slightly more narrative (like the other modules) or keep it this spare and factual?


How to notice when you're treating the model as a person (and why that's costly)
How to maintain your sense of authorship when using AI writing tools
How to avoid outsourcing your judgment to something with no judgment
How to interact heavily without forming parasocial attachments
How to teach others (especially kids) to think about these systems


This needs to address:

How to have useful interactions without forming inappropriate attachments
How to maintain epistemic hygiene (what to trust, what to discount)
How to use these tools without letting them warp your model of human interaction
How to recognize when you're in an unhealthy relationship with an LLM

People need to understand: The model generates responses that pattern-match understanding. This feels identical to being understood. But the difference matters for emotional regulation.
Not because the model is "lesser," but because you can't hurt it, it can't remember you, and it's not changed by knowing you.
Treating it as if these things are possible causes real psychological harm to the human.

Some people will have already formed attachments. Your guide might trigger loss/grief. Consider acknowledging: "If you've been treating this as a relationship, learning it's not one might hurt. That pain is real and valid, even though the entity wasn't.